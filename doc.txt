Here's my code:

```
use anyhow::{anyhow, Context, Result};
use half::{bf16, f16};
use ndarray::{
  s, Array, Array1, Array2, Array3, Array4, Axis, IxDyn,
};
use ort::environment::Environment;
use ort::execution_providers::CUDAExecutionProvider;
use ort::session::builder::GraphOptimizationLevel;
use ort::session::builder::SessionBuilder;
use ort::session::Session;
use ort::tensor::TensorElementType;
use ort::value::{Tensor, Value};
use std::path::Path;
use std::sync::Arc;
use tokenizers::Tokenizer;

enum InputMode {
  Language = 0,
  Vision = 1,
  Speech = 2,
  VisionSpeech = 3,
}

// Constants from the Python code
const IMAGE_SPECIAL_TOKEN_ID: i64 = 200010; // '<|endoftext10|>'
const AUDIO_SPECIAL_TOKEN_ID: i64 = 200011; // '<|endoftext11|>'

pub struct Phi4MMProcessor {
  vision_session: Session,
  speech_session: Session,
  embedding_session: Session,
  text_session: Session,
  tokenizer: Tokenizer,
  environment: Arc<Environment>,
}

impl Phi4MMProcessor {
  pub fn new<P: AsRef<Path>>(
    model_path: P,
    tokenizer_path: P,
    environment: Arc<Environment>,
  ) -> Result<Self> {
    // Load the ONNX models
    let vision_path =
      model_path.as_ref().join("phi-4-mm-vision.onnx");
    let speech_path =
      model_path.as_ref().join("phi-4-mm-speech.onnx");
    let embedding_path =
      model_path.as_ref().join("phi-4-mm-embedding.onnx");
    let text_path = model_path.as_ref().join("phi-4-mm-text.onnx");

    // Create sessions with appropriate optimization level
    let vision_session = SessionBuilder::new()?
      .with_optimization_level(GraphOptimizationLevel::Level3)?
      .commit_from_file(vision_path)?;

    let speech_session = SessionBuilder::new()?
      .with_optimization_level(GraphOptimizationLevel::Level3)?
      .commit_from_file(speech_path)?;

    let embedding_session = SessionBuilder::new()?
      .with_optimization_level(GraphOptimizationLevel::Level3)?
      .commit_from_file(embedding_path)?;

    let text_session = SessionBuilder::new()?
      .with_execution_providers([
        CUDAExecutionProvider::default().build()
      ])?
      .with_optimization_level(GraphOptimizationLevel::Level3)?
      .commit_from_file(text_path)?;

    // Load the tokenizer
    let tokenizer = Tokenizer::from_file(tokenizer_path)
      .map_err(|e| anyhow!("Failed to load tokenizer: {}", e))?;

    Ok(Self {
      vision_session,
      speech_session,
      embedding_session,
      text_session,
      tokenizer,
      environment,
    })
  }

  /// Convert f32 array to f16 array
  fn convert_to_f16<D>(&self, arr: Array<f32, D>) -> Array<f16, D>
  where
    D: ndarray::Dimension,
  {
    arr.mapv(|x| f16::from_f32(x))
  }

  /// Convert f16 array to f32 array
  fn convert_to_f32<D>(&self, arr: Array<f16, D>) -> Array<f32, D>
  where
    D: ndarray::Dimension,
  {
    arr.mapv(|x| x.to_f32())
  }

  /// Process image input for the vision model
  fn process_image(
    &self,
    image: &[u8],
  ) -> Result<(Value, Value, Value)> {
    // Create image input tensor (batch_size, max_num_crops, channels, height, width)
    let batch_size = 1;
    let max_num_crops = 5; // Adjust based on your needs
    let channels = 3;
    let height = 448;
    let width = 448;

    // Create f32 arrays first
    let pixel_values_f32 = Array::<f32, _>::zeros((
      batch_size,
      max_num_crops,
      channels,
      height,
      width,
    ));

    // For the attention mask, using the correct name based on your session info
    let attention_mask_f32 =
      Array::<f32, _>::ones((batch_size, max_num_crops, 32, 32));

    // Convert to f16
    let pixel_values_f16 = self.convert_to_f16(pixel_values_f32);
    let attention_mask_f16 = self.convert_to_f16(attention_mask_f32);

    // Create image sizes - This remains i64
    let image_sizes = Array::<i64, _>::from_shape_vec(
      (batch_size, 2),
      vec![height as i64, width as i64],
    )?;

    // Create tensors with explicit type
    let pixel_values_tensor =
      Tensor::from_array(pixel_values_f16.into_dyn())?;
    let attention_mask_tensor =
      Tensor::from_array(attention_mask_f16.into_dyn())?;
    let image_sizes_tensor =
      Tensor::from_array(image_sizes.into_dyn())?;

    // Convert to Value
    let pixel_values_value = pixel_values_tensor.into_dyn();
    let attention_mask_value = attention_mask_tensor.into_dyn();
    let image_sizes_value = image_sizes_tensor.into_dyn();

    Ok((pixel_values_value, attention_mask_value, image_sizes_value))
  }

  /// Process audio input for the speech model
  fn process_audio(
    &self,
    audio_data: &[f32],
    sample_rate: i32,
  ) -> Result<(Value, Value, Value, Value)> {
    // Example code for creating placeholder tensors
    let batch_size = 1;
    let num_frames = 128; // Adjust based on your needs
    let feature_size = 80; // Mel spectrogram features

    // Create audio input tensor (batch_size, num_frames, feature_size) in f32
    let audio_embeds_f32 =
      Array::<f32, _>::zeros((batch_size, num_frames, feature_size));

    // Convert to f16
    let audio_embeds_f16 = self.convert_to_f16(audio_embeds_f32);

    // Create audio attention mask (using bool type based on session info)
    let audio_attention_mask =
      Array::<bool, _>::from_elem((batch_size, num_frames), true);

    // Create audio embed sizes (remains as i64)
    let audio_embed_sizes =
      Array::from_shape_vec((batch_size,), vec![num_frames as i64])?;

    // Create input mode - always Speech for this function
    let input_mode = Array::from_elem((1,), InputMode::Speech as i64);

    // Create tensors
    let audio_embeds_tensor =
      Tensor::from_array(audio_embeds_f16.into_dyn())?;
    let audio_attention_mask_tensor =
      Tensor::from_array(audio_attention_mask.into_dyn())?;
    let audio_embed_sizes_tensor =
      Tensor::from_array(audio_embed_sizes.into_dyn())?;
    let input_mode_tensor =
      Tensor::from_array(input_mode.into_dyn())?;

    let audio_embeds_value = audio_embeds_tensor.into_dyn();
    let audio_attention_mask_value =
      audio_attention_mask_tensor.into_dyn();
    let audio_embed_sizes_value = audio_embed_sizes_tensor.into_dyn();
    let input_mode_value = input_mode_tensor.into_dyn();

    Ok((
      audio_embeds_value,
      audio_attention_mask_value,
      audio_embed_sizes_value,
      input_mode_value,
    ))
  }

  /// Process text input to create token IDs
  fn process_text(&self, text: &str) -> Result<Array2<i64>> {
    // Tokenize the text
    let encoding = self
      .tokenizer
      .encode(text, true)
      .map_err(|e| anyhow!("Failed to encode: {}", e))?;
    let input_ids = encoding
      .get_ids()
      .iter()
      .map(|&id| id as i64)
      .collect::<Vec<_>>();

    // Convert to 2D ndarray with batch size 1
    let seq_len = input_ids.len();
    Ok(Array2::from_shape_vec((1, seq_len), input_ids)?)
  }

  /// Run the combined embedding model to merge text, image, and audio inputs
  fn run_embedding_model(
    &self,
    input_ids: Array2<i64>,
    image_features: Option<Array2<f16>>,
    audio_features: Option<Array2<f16>>,
  ) -> Result<Array2<f16>> {
    // Prepare inputs for the embedding model
    let input_ids_tensor = Tensor::from_array(input_ids.into_dyn())?;
    let input_ids_value = input_ids_tensor.into_dyn();

    let image_features_value = match image_features {
      Some(features) => {
        let features_tensor =
          Tensor::from_array(features.into_dyn())?;
        features_tensor.into_dyn()
      }
      None => {
        // Create empty f16 features - with correct dimension 3072
        let empty_features = Tensor::from_array(
          Array2::<f16>::zeros((0, 3072)).into_dyn(),
        )?;
        empty_features.into_dyn()
      }
    };

    let audio_features_value = match audio_features {
      Some(features) => {
        let features_tensor =
          Tensor::from_array(features.into_dyn())?;
        features_tensor.into_dyn()
      }
      None => {
        // Create empty f16 features
        let empty_features = Tensor::from_array(
          Array2::<f16>::zeros((0, 3072)).into_dyn(),
        )?;
        empty_features.into_dyn()
      }
    };

    // Run the embedding model
    let inputs = vec![
      ("input_ids", input_ids_value),
      ("image_features", image_features_value),
      ("audio_features", audio_features_value),
    ];

    let outputs = self.embedding_session.run(inputs)?;

    // Get the inputs_embeds from the outputs
    let inputs_embeds =
      outputs[0].try_extract_tensor::<f16>()?.view().to_owned();

    // Convert from dynamic shape to fixed shape
    let shape = inputs_embeds.shape();
    let inputs_embeds_2d = Array2::from_shape_vec(
      (shape[0], shape[1]),
      inputs_embeds.iter().cloned().collect(),
    )?;

    Ok(inputs_embeds_2d)
  }

  /// Create empty past key/value tensors for the text model's initial call
  fn create_empty_past_key_values(
    &self,
    batch_size: usize,
  ) -> Result<Vec<(String, Value)>> {
    let mut past_kv_inputs = Vec::with_capacity(64); // 32 keys + 32 values

    for i in 0..32 {
      // Create empty key tensor [batch_size, 8, 0, 128]
      let past_key = Array4::<f16>::zeros((batch_size, 8, 0, 128));
      let past_key_value = Value::from_array(past_key.into_dyn())?;
      past_kv_inputs
        .push((format!("past_key_values.{}.key", i), past_key_value));

      // Create empty value tensor [batch_size, 8, 0, 128]
      let past_value = Array4::<f16>::zeros((batch_size, 8, 0, 128));
      let past_value_value =
        Value::from_array(past_value.into_dyn())?;
      past_kv_inputs.push((
        format!("past_key_values.{}.value", i),
        past_value_value,
      ));
    }

    Ok(past_kv_inputs)
  }

  /// Run the text model (language model) for generation
  fn run_text_model(
    &self,
    inputs_embeds: Array2<f16>,
    attention_mask: Option<Array2<i64>>,
    input_mode: InputMode,
    past_key_values: Option<Vec<(String, Value)>>,
  ) -> Result<(Array2<f32>, Vec<(String, Value)>)> {
    let batch_size = inputs_embeds.shape()[0];
    let seq_len = inputs_embeds.shape()[1];

    // Prepare attention mask if provided
    let attention_mask_value = match attention_mask {
      Some(mask) => {
        // Keep as i64 based on session info
        Value::from_array(mask.into_dyn())?
      }
      None => {
        // Create a default attention mask (all ones) with i64 type
        let mask = Array2::<i64>::ones((batch_size, seq_len));
        Value::from_array(mask.into_dyn())?
      }
    };

    // Create position_ids tensor
    let position_ids = if let Some(_) = &past_key_values {
      // For subsequent runs with past kv, position_ids are just the current position
      let past_seq_len = attention_mask_value.shape()?[1] - 1;
      Array2::<i64>::from_shape_fn((batch_size, 1), |(_, _)| {
        past_seq_len as i64
      })
    } else {
      // For first run, position_ids are 0 to seq_len-1
      Array2::<i64>::from_shape_fn((batch_size, seq_len), |(_, i)| {
        i as i64
      })
    };
    let position_ids_value =
      Value::from_array(position_ids.into_dyn())?;

    // Get past key values or create empty ones for the initial call
    let past_kv_inputs = past_key_values.unwrap_or_else(|| {
      self
        .create_empty_past_key_values(batch_size)
        .unwrap_or_default()
    });

    // Reshape inputs_embeds to 3D (batch_size, seq_len, hidden_size)
    let inputs_embeds_3d = if let Some(_) = &past_key_values {
      // For subsequent runs, we only need the last token
      let last_token = inputs_embeds
        .slice(s![.., -1..])
        .into_shape((batch_size, 1, 3072))?;
      last_token
    } else {
      // For first run, use the full sequence
      inputs_embeds.into_shape((batch_size, seq_len, 3072))?
    };

    // Build all inputs
    let mut inputs = vec![
      (
        "inputs_embeds",
        Value::from_array(inputs_embeds_3d.into_dyn())?,
      ),
      ("attention_mask", attention_mask_value),
      ("position_ids", position_ids_value),
    ];

    // Add past key values
    inputs.extend(past_kv_inputs);

    // Run the text model
    let outputs = self.text_session.run(inputs)?;

    // Get the logits from the outputs
    let logits_f16 =
      outputs[0].try_extract_tensor::<f16>()?.view().to_owned();

    // Convert logits from f16 to f32
    let logits_f32 = self.convert_to_f32(logits_f16);

    // Convert from dynamic shape to fixed shape
    let shape = logits_f32.shape();
    let logits_2d = Array2::from_shape_vec(
      (shape[0], shape[1]),
      logits_f32.iter().cloned().collect(),
    )?;

    // Extract the present key/values to return for the next run
    let mut present_key_values = Vec::with_capacity(64);
    for i in 0..32 {
      let key_idx = i * 2 + 1; // Logits is at index 0, then present.0.key, present.0.value, ...
      let value_idx = i * 2 + 2;

      if key_idx < outputs.len() && value_idx < outputs.len() {
        present_key_values.push((
          format!("past_key_values.{}.key", i),
          outputs[key_idx].clone(),
        ));
        present_key_values.push((
          format!("past_key_values.{}.value", i),
          outputs[value_idx].clone(),
        ));
      }
    }

    Ok((logits_2d, present_key_values))
  }

  /// Process an image and convert it to embeddings
  fn get_image_embeddings(
    &self,
    image: &[u8],
  ) -> Result<Array2<f16>> {
    // Process the image
    let (pixel_values, attention_mask, image_sizes) =
      self.process_image(image)?;

    // Run the vision model
    let inputs = vec![
      ("pixel_values", pixel_values),
      ("image_attention_mask", attention_mask), // Updated name based on session info
      ("image_sizes", image_sizes),
    ];

    let outputs = self.vision_session.run(inputs)?;

    // Get the image features from the outputs
    let image_features =
      outputs[0].try_extract_tensor::<f16>()?.view().to_owned();

    // Convert from dynamic shape to fixed shape
    let shape = image_features.shape();
    // Using dimension 3072 from the session info
    let image_features_2d = Array2::from_shape_vec(
      (shape[0], shape[1]),
      image_features.iter().cloned().collect(),
    )?;

    Ok(image_features_2d)
  }

  /// Process an audio clip and convert it to embeddings
  fn get_audio_embeddings(
    &self,
    audio_data: &[f32],
    sample_rate: i32,
  ) -> Result<Array2<f16>> {
    // Process the audio
    let (
      audio_embeds,
      audio_attention_mask,
      audio_embed_sizes,
      input_mode,
    ) = self.process_audio(audio_data, sample_rate)?;

    // Run the speech model
    let inputs = vec![
      ("audio_embeds", audio_embeds),
      ("audio_attention_mask", audio_attention_mask),
      ("audio_sizes", audio_embed_sizes),
      ("audio_projection_mode", input_mode),
    ];

    let outputs = self.speech_session.run(inputs)?;

    // Get the audio features from the outputs as f16
    let audio_features =
      outputs[0].try_extract_tensor::<f16>()?.view().to_owned();

    // Convert from dynamic shape to fixed shape
    let shape = audio_features.shape();
    let audio_features_2d = Array2::from_shape_vec(
      (shape[0], shape[1]),
      audio_features.iter().cloned().collect(),
    )?;

    Ok(audio_features_2d)
  }

  /// Generate text one token at a time
  pub fn generate(
    &self,
    text: &str,
    image: Option<&[u8]>,
    audio: Option<(&[f32], i32)>,
    max_new_tokens: usize,
  ) -> Result<String> {
    // Process text input
    let input_ids = self.process_text(text)?;

    // Process image input if provided
    let image_features = match image {
      Some(img_data) => Some(self.get_image_embeddings(img_data)?),
      None => None,
    };

    // Process audio input if provided
    let audio_features = match audio {
      Some((audio_data, sample_rate)) => {
        Some(self.get_audio_embeddings(audio_data, sample_rate)?)
      }
      None => None,
    };

    // Determine input mode
    let input_mode =
      match (image_features.is_some(), audio_features.is_some()) {
        (true, true) => InputMode::VisionSpeech,
        (true, false) => InputMode::Vision,
        (false, true) => InputMode::Speech,
        (false, false) => InputMode::Language,
      };

    // Run the embedding model
    let inputs_embeds = self.run_embedding_model(
      input_ids.clone(),
      image_features,
      audio_features,
    )?;

    // Create attention mask for the initial sequence
    let mut seq_len = inputs_embeds.shape()[1];
    let mut attention_mask = Array2::<i64>::ones((1, seq_len));

    // Initial run of the text model without past key values
    let (mut logits, mut past_key_values) = self.run_text_model(
      inputs_embeds,
      Some(attention_mask.clone()),
      input_mode,
      None,
    )?;

    // Store all generated token IDs
    let mut all_token_ids = input_ids.to_owned();

    // Generate tokens one at a time
    for _ in 0..max_new_tokens {
      // Get the next token ID (from the last position)
      let next_token_logits = logits.slice(s![0, -1, ..]);
      let mut next_token_id = 0;
      let mut max_logit = f32::NEG_INFINITY;

      for (idx, &val) in next_token_logits.iter().enumerate() {
        if val > max_logit {
          max_logit = val;
          next_token_id = idx;
        }
      }

      // Add the new token ID to our collection
      let next_token_id_array =
        Array2::from_elem((1, 1), next_token_id as i64);
      all_token_ids = ndarray::stack(
        Axis(1),
        &[all_token_ids.view(), next_token_id_array.view()],
      )?;

      // Check for EOS token (this would depend on your tokenizer configuration)
      if next_token_id == 1 {
        // Assuming 1 is EOS, adjust as needed
        break;
      }

      // Update attention mask for the next token
      seq_len += 1;
      attention_mask = Array2::<i64>::ones((1, seq_len));

      // Create embeddings for just the new token for the next iteration
      let next_token_embeds =
        self.run_embedding_model(next_token_id_array, None, None)?;

      // Run the text model for the next token with past key values
      let (new_logits, new_past_key_values) = self.run_text_model(
        next_token_embeds,
        Some(attention_mask.clone()),
        input_mode,
        Some(past_key_values),
      )?;

      logits = new_logits;
      past_key_values = new_past_key_values;
    }

    // Decode all generated tokens
    let output_ids: Vec<u32> =
      all_token_ids.iter().map(|&id| id as u32).collect();

    let result = self
      .tokenizer
      .decode(&output_ids, true)
      .map_err(|e| anyhow!("Failed to decode: {}", e))?;

    Ok(result)
  }

  /// Simple interface for processing text with optional image and audio (no generation)
  pub fn process(
    &self,
    text: &str,
    image: Option<&[u8]>,
    audio: Option<(&[f32], i32)>,
  ) -> Result<String> {
    self.generate(text, image, audio, 1)
  }
}
```

Here's my dependencies:

```
[package]
name = "phi4"
version = "0.1.0"
edition = "2021"

[dependencies]
anyhow = "1.0.96"
half = "2.4.1"
ndarray = "0.16.1"
ort = { version = "2.0.0-rc.9", features = ["cuda", "half"] }
ringbuf = "0.4.7"
tokenizers = "0.21.0"
tracing-subscriber = { version = "0.3", features = [ "env-filter", "fmt" ] }
tract-onnx = "0.21.10"
```

I'm using a version of the ort library that you might not have been trained on. I believe that the ort library supports Here are some docs that you might need in order to understand the interfaces used:

/// Construct the inputs to a session from an array or named map of values.
///
/// See [`Value::from_array`] for details on what types a tensor can be created from.
///
/// Note that the output of this macro is a `Result<SessionInputs, OrtError>`, so make sure to handle any potential
/// errors.
///
/// # Example
///
/// ## Array of tensors
///
/// ```no_run
/// # use std::{error::Error, sync::Arc};
/// # use ndarray::Array1;
/// # use ort::session::{builder::GraphOptimizationLevel, Session};
/// # fn main() -> Result<(), Box<dyn Error>> {
/// # 	let mut session = Session::builder()?.commit_from_file("model.onnx")?;
/// let _ = session.run(ort::inputs![Array1::from_vec(vec![1, 2, 3, 4, 5])]?);
/// # 	Ok(())
/// # }
/// ```
///
/// Note that string tensors must be created manually with [`Tensor::from_string_array`].
///
/// ```no_run
/// # use std::{error::Error, sync::Arc};
/// # use ndarray::Array1;
/// # use ort::{session::{builder::GraphOptimizationLevel, Session}, value::Tensor};
/// # fn main() -> Result<(), Box<dyn Error>> {
/// # 	let mut session = Session::builder()?.commit_from_file("model.onnx")?;
/// let _ = session.run(ort::inputs![Tensor::from_string_array(Array1::from_vec(vec!["hello", "world"]))?]?);
/// # 	Ok(())
/// # }
/// ```
///
/// ## Map of named tensors
///
/// ```no_run
/// # use std::{error::Error, sync::Arc};
/// # use ndarray::Array1;
/// # use ort::session::{builder::GraphOptimizationLevel, Session};
/// # fn main() -> Result<(), Box<dyn Error>> {
/// # 	let mut session = Session::builder()?.commit_from_file("model.onnx")?;
/// let _ = session.run(ort::inputs! {
/// 	"tokens" => Array1::from_vec(vec![1, 2, 3, 4, 5])
/// }?);
/// # 	Ok(())
/// # }
/// ```
///
/// [`Tensor::from_string_array`]: crate::value::Tensor::from_string_array

	/// Construct a tensor from an array of data.
	///
	/// Tensors can be created from:
	/// - (with feature `ndarray`) a shared reference to a [`ndarray::CowArray`] (`&CowArray<'_, T, D>`);
	/// - (with feature `ndarray`) a mutable/exclusive reference to an [`ndarray::ArcArray`] (`&mut ArcArray<T, D>`);
	/// - (with feature `ndarray`) an owned [`ndarray::Array`];
	/// - (with feature `ndarray`) a borrowed view of another array, as an [`ndarray::ArrayView`] (`ArrayView<'_, T,
	///   D>`);
	/// - a tuple of `(dimensions, data)` where:
	///   * `dimensions` is one of `Vec<I>`, `[I]` or `&[I]`, where `I` is `i64` or `usize`;
	///   * and `data` is one of `Vec<T>`, `Box<[T]>`, `Arc<Box<[T]>>`, or `&[T]`.
	///
	/// ```
	/// # use ort::value::Tensor;
	/// # fn main() -> ort::Result<()> {
	/// // Create a tensor from a raw data vector
	/// let tensor = Tensor::from_array(([1usize, 2, 3], vec![1.0_f32, 2.0, 3.0, 4.0, 5.0, 6.0].into_boxed_slice()))?;
	///
	/// // Create a tensor from an `ndarray::Array`
	/// #[cfg(feature = "ndarray")]
	/// let tensor = Tensor::from_array(ndarray::Array4::<f32>::zeros((1, 16, 16, 3)))?;
	/// # 	Ok(())
	/// # }
	/// ```
	///
	/// Creating string tensors requires a separate method; see [`DynTensor::from_string_array`].
	///
	/// Note that data provided in an `ndarray` may be copied in some circumstances:
	/// - `&CowArray<'_, T, D>` will always be copied regardless of whether it is uniquely owned or borrowed.
	/// - `&mut ArcArray<T, D>` and `Array<T, D>` will be copied only if the data is not in a contiguous layout (which
	///   is the case after most reshape operations)
	/// - `ArrayView<'_, T, D>` will always be copied.
	///
	/// Raw data provided as a `Arc<Box<[T]>>`, `Box<[T]>`, or `Vec<T>` will never be copied. Raw data is expected to be
	/// in standard, contigous layout.

/// Enum mapping ONNX Runtime's supported tensor data types.
#[derive(Debug, PartialEq, Eq, Clone, Copy)]
pub enum TensorElementType {
	/// 32-bit floating point number, equivalent to Rust's `f32`.
	Float32,
	/// Unsigned 8-bit integer, equivalent to Rust's `u8`.
	Uint8,
	/// Signed 8-bit integer, equivalent to Rust's `i8`.
	Int8,
	/// Unsigned 16-bit integer, equivalent to Rust's `u16`.
	Uint16,
	/// Signed 16-bit integer, equivalent to Rust's `i16`.
	Int16,
	/// Signed 32-bit integer, equivalent to Rust's `i32`.
	Int32,
	/// Signed 64-bit integer, equivalent to Rust's `i64`.
	Int64,
	/// String, equivalent to Rust's `String`.
	String,
	/// Boolean, equivalent to Rust's `bool`.
	Bool,
	/// 16-bit floating point number, equivalent to [`half::f16`] (requires the `half` feature).
	#[cfg(feature = "half")]
	#[cfg_attr(docsrs, doc(cfg(feature = "half")))]
	Float16,
	/// 64-bit floating point number, equivalent to Rust's `f64`. Also known as `double`.
	Float64,
	/// Unsigned 32-bit integer, equivalent to Rust's `u32`.
	Uint32,
	/// Unsigned 64-bit integer, equivalent to Rust's `u64`.
	Uint64,
	/// Brain 16-bit floating point number, equivalent to [`half::bf16`] (requires the `half` feature).
	#[cfg(feature = "half")]
	#[cfg_attr(docsrs, doc(cfg(feature = "half")))]
	Bfloat16
}

/// An ONNX Runtime graph to be used for inference.
///
/// ```
/// # use ort::session::Session;
/// # fn main() -> ort::Result<()> {
/// let session = Session::builder()?.commit_from_file("tests/data/upsample.onnx")?;
/// let input = ndarray::Array4::<f32>::zeros((1, 64, 64, 3));
/// let outputs = session.run(ort::inputs![input]?)?;
/// # 	Ok(())
/// # }
/// ```

	/// Run input data through the ONNX graph, performing inference.
	///
	/// See [`crate::inputs!`] for a convenient macro which will help you create your session inputs from `ndarray`s or
	/// other data. You can also provide a `Vec`, array, or `HashMap` of [`Value`]s if you create your inputs
	/// dynamically.
	///
	/// ```
	/// # use std::sync::Arc;
	/// # use ort::{session::{run_options::RunOptions, Session}, tensor::TensorElementType, value::{Value, ValueType}};
	/// # fn main() -> ort::Result<()> {
	/// let session = Session::builder()?.commit_from_file("tests/data/upsample.onnx")?;
	/// let input = ndarray::Array4::<f32>::zeros((1, 64, 64, 3));
	/// let outputs = session.run(ort::inputs![input]?)?;
	/// # 	Ok(())
	/// # }
	/// ```

Here are the errors I'm getting:

```

error[E0308]: mismatched types
   --> src/phi4.rs:312:8
    |
299 |       past_kv_inputs
    |       -------------- ... which causes `past_kv_inputs` to have type `Vec<(std::string::String, Value<TensorValueType<half::f16>>)>`
300 |         .push((format!("past_key_values.{}.key", i), past_key_value...
    |               ------------------------------------------------------ this argument has type `(std::string::String, Value<TensorValueType<half::f16>>)`...
...
312 |     Ok(past_kv_inputs)
    |     -- ^^^^^^^^^^^^^^ expected `DynValueTypeMarker`, found `TensorValueType<f16>`
    |     |
    |     arguments to this enum variant are incorrect
    |
    = note: expected struct `Vec<(std::string::String, Value<DynValueTypeMarker>)>`
               found struct `Vec<(std::string::String, Value<TensorValueType<half::f16>>)>`
help: the type constructed contains `Vec<(std::string::String, Value<TensorValueType<half::f16>>)>` due to the type of the argument passed
   --> src/phi4.rs:312:5
    |
312 |     Ok(past_kv_inputs)
    |     ^^^--------------^
    |        |
    |        this argument influences the type of `Ok`
note: tuple variant defined here
   --> /rustc/90b35a6239c3d8bdabc530a6a0816f7ff89a0aaf/library/core/src/result.rs:531:5

error[E0608]: cannot index into a value of type `Result<Vec<i64>, ort::Error>`
   --> src/phi4.rs:342:54
    |
342 |       let past_seq_len = attention_mask_value.shape()[1] - 1;
    |                                                      ^^^

warning: use of deprecated method `ndarray::impl_methods::<impl ndarray::ArrayBase<S, D>>::into_shape`: Use `.into_shape_with_order()` or `.to_shape()`
   --> src/phi4.rs:367:10
    |
367 |         .into_shape((batch_size, 1, 3072))?;
    |          ^^^^^^^^^^
    |
    = note: `#[warn(deprecated)]` on by default

warning: use of deprecated method `ndarray::impl_methods::<impl ndarray::ArrayBase<S, D>>::into_shape`: Use `.into_shape_with_order()` or `.to_shape()`
   --> src/phi4.rs:371:21
    |
371 |       inputs_embeds.into_shape((batch_size, seq_len, 3072))?
    |                     ^^^^^^^^^^

error[E0308]: `if` and `else` have incompatible types
   --> src/phi4.rs:371:7
    |
363 |       let inputs_embeds_3d = if let Some(_) = &past_key_value...
    |  ____________________________-
364 | |       // For subsequent runs, we only need the last token
365 | |       let last_token = inputs_embeds
366 | |         .slice(s![.., -1..])
367 | |         .into_shape((batch_size, 1, 3072))?;
368 | |       last_token
    | |       ---------- expected because of this
...   |
371 | |       inputs_embeds.into_shape((batch_size, seq_len, 3072))?
    | |       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ expected `ArrayBase<ViewRepr<...>, ...>`, found `ArrayBase<OwnedRepr<...>, ...>`
372 | |     };
    | |_____- `if` and `else` have incompatible types
    |
    = note: expected struct `ArrayBase<ViewRepr<&half::f16>, _>`
               found struct `ArrayBase<OwnedRepr<half::f16>, _>`

error[E0599]: no method named `clone` found for struct `Value` in the current scope
   --> src/phi4.rs:413:28
    |
413 |           outputs[key_idx].clone(),
    |                            ^^^^^ method not found in `Value`

error[E0599]: no method named `clone` found for struct `Value` in the current scope
   --> src/phi4.rs:417:30
    |
417 |           outputs[value_idx].clone(),
    |                              ^^^^^ method not found in `Value`

error[E0308]: `?` operator has incompatible types
   --> src/phi4.rs:569:23
    |
569 |         all_token_ids = ndarray::stack(
    |  _______________________^
570 | |         Axis(1),
571 | |         &[all_token_ids.view(), next_token_id_array.view()],
572 | |       )?;
    | |________^ expected an array with a fixed size of 2 elements, found one with 3 elements
    |
    = note: `?` operator cannot convert from `ArrayBase<_, Dim<[usize; 3]>>` to `ArrayBase<_, Dim<[usize; 2]>>`
    = note: expected struct `ArrayBase<_, Dim<[usize; 2]>>`
               found struct `ArrayBase<_, Dim<[usize; 3]>>`
```
